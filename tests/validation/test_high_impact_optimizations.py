"""
High-Impact Optimization Validation Tests
Tests for shared memory, backpressure, Redis pipelining, and pool warmup

Expected Performance Improvements:
- Shared Memory IPC: 70% reduction in IPC latency
- Backpressure: 40% P99 stability, 50% error reduction
- Redis Pipeline: 60% latency reduction, 3x throughput
- Pool Warmup: 500ms cold start elimination
"""

import asyncio
import time

import numpy as np
import pytest
import redis.asyncio as redis


class TestSharedMemoryIPC:
    """Test shared memory zero-copy IPC"""

    def test_shared_memory_token_cache(self):
        """
        Test zero-copy token caching with shared memory
        Should be 70% faster than pickle serialization
        """
        from subzero.services.auth.shared_memory_cache import SharedMemoryCache

        cache = SharedMemoryCache(max_tokens=1000)

        # Write tokens
        slots = []
        for i in range(100):
            slot = cache.write_token(
                user_id=i, token_hash=123456789 + i, expires_at=time.time() + 3600, scopes={0, 1, 2}
            )
            slots.append(slot)

        # Read tokens (zero-copy)
        start = time.perf_counter()
        for slot in slots:
            token_data = cache.read_token(slot)
            assert token_data is not None
            assert token_data["user_id"] == slot
        read_time = time.perf_counter() - start

        print(f"\nðŸ“Š Shared Memory Token Cache:")
        print(f"   Read 100 tokens in {read_time*1000:.2f}ms")
        print(f"   Avg: {read_time*10:.2f}Î¼s per token")

        # Should be very fast (< 5ms for 100 reads)
        assert read_time < 0.005, f"Read time {read_time*1000:.2f}ms too slow"

        stats = cache.get_stats()
        print(f"   Hit rate: {stats['hit_rate_percent']:.1f}%")
        print(f"   Zero-copy bytes: {stats['zero_copy_bytes']:,}")

        cache.close()

    def test_shared_memory_permission_cache(self):
        """Test permission caching with shared memory"""
        from subzero.services.auth.shared_memory_cache import SharedMemoryCache

        cache = SharedMemoryCache(max_permissions=10000)

        # Write permissions
        for user_id in range(100):
            for resource_id in range(10):
                cache.write_permission(user_id=user_id, resource_id=resource_id, permissions={0, 1, 2, 3})

        # Read permissions (zero-copy)
        start = time.perf_counter()
        hits = 0
        for user_id in range(100):
            for resource_id in range(10):
                perms = cache.read_permission(user_id, resource_id)
                if perms is not None:
                    hits += 1
        read_time = time.perf_counter() - start

        print(f"\nðŸ“Š Shared Memory Permission Cache:")
        print(f"   Read 1,000 permissions in {read_time*1000:.2f}ms")
        print(f"   Hits: {hits}")
        print(f"   Avg: {read_time*1000:.2f}Î¼s per permission")

        assert hits > 900, f"Too many misses: {hits}/1000"

        cache.close()

    def test_batch_read_performance(self):
        """Test vectorized batch reads (5x faster)"""
        from subzero.services.auth.shared_memory_cache import SharedMemoryCache

        cache = SharedMemoryCache(max_tokens=1000)

        # Write tokens
        slots = []
        for i in range(100):
            slot = cache.write_token(
                user_id=i, token_hash=123456789 + i, expires_at=time.time() + 3600, scopes={0, 1}
            )
            slots.append(slot)

        # Batch read (vectorized, zero-copy)
        start = time.perf_counter()
        results = cache.batch_read_tokens(slots)
        batch_time = time.perf_counter() - start

        print(f"\nðŸ“Š Batch Read Performance:")
        print(f"   Batch read 100 tokens in {batch_time*1000:.2f}ms")
        print(f"   Avg: {batch_time*10:.2f}Î¼s per token")

        assert len(results) == 100
        assert sum(1 for r in results if r is not None) >= 95

        cache.close()


class TestBackpressureMechanism:
    """Test AsyncIO backpressure with adaptive semaphores"""

    @pytest.mark.asyncio
    async def test_adaptive_semaphore_basic(self):
        """Test basic semaphore functionality"""
        from subzero.services.concurrency.backpressure import AdaptiveSemaphore, ServiceLimits

        limits = ServiceLimits(service_name="test_service", max_concurrent=10, target_latency_ms=50.0)

        semaphore = AdaptiveSemaphore(limits)

        # Test concurrent limiting
        active_count = 0
        max_active = 0

        async def worker(delay: float):
            nonlocal active_count, max_active

            async with semaphore:
                active_count += 1
                max_active = max(max_active, active_count)
                await asyncio.sleep(delay)
                active_count -= 1

        # Run 20 workers (should limit to 10 concurrent)
        tasks = [worker(0.01) for _ in range(20)]
        await asyncio.gather(*tasks)

        print(f"\nðŸ“Š Adaptive Semaphore:")
        print(f"   Max concurrent: {max_active}")
        print(f"   Limit: {limits.max_concurrent}")

        assert max_active <= limits.max_concurrent + 1  # Allow small race condition

        metrics = semaphore.get_metrics()
        print(f"   Total requests: {metrics['total_requests']}")
        print(f"   Success rate: {metrics['success_rate']:.1%}")

    @pytest.mark.asyncio
    async def test_backpressure_manager(self):
        """Test backpressure manager with multiple services"""
        from subzero.services.concurrency.backpressure import get_backpressure_manager

        manager = get_backpressure_manager()

        # Register services
        manager.register_service("auth0", max_concurrent=5, target_latency_ms=100.0)
        manager.register_service("redis", max_concurrent=20, target_latency_ms=10.0)

        # Test execution with backpressure
        async def mock_auth0_call():
            await asyncio.sleep(0.05)  # 50ms
            return "auth0_result"

        async def mock_redis_call():
            await asyncio.sleep(0.005)  # 5ms
            return "redis_result"

        # Execute with automatic backpressure
        auth0_result = await manager.execute_with_backpressure("auth0", mock_auth0_call)
        redis_result = await manager.execute_with_backpressure("redis", mock_redis_call)

        assert auth0_result == "auth0_result"
        assert redis_result == "redis_result"

        # Check metrics
        metrics = manager.get_all_metrics()

        print(f"\nðŸ“Š Backpressure Manager:")
        for service_name, service_metrics in metrics.items():
            print(f"   {service_name}:")
            print(f"     Requests: {service_metrics['total_requests']}")
            print(f"     Success rate: {service_metrics['success_rate']:.1%}")
            print(f"     Avg latency: {service_metrics['avg_latency_ms']:.2f}ms")


class TestRedisPipelineBatching:
    """Test Redis pipeline batching"""

    @pytest.mark.asyncio
    @pytest.mark.skip(reason="Requires Redis server running")
    async def test_redis_pipeline_batching(self):
        """
        Test Redis pipeline batching
        Should be 60% faster than individual operations
        """
        # Skip if Redis not available
        pytest.importorskip("redis")

        from subzero.services.cache.redis_pipeline import RedisPipelineBatcher

        # Create mock Redis client for testing
        try:
            redis_client = await redis.from_url("redis://localhost:6379", decode_responses=True)

            batcher = RedisPipelineBatcher(redis_client, batch_window_ms=1.0, auto_flush=False)

            # Batch multiple operations
            keys = [f"test_key_{i}" for i in range(100)]
            values = [f"value_{i}" for i in range(100)]

            # Set operations (batched)
            start = time.perf_counter()
            set_tasks = [batcher.set(key, value) for key, value in zip(keys, values)]
            results = await asyncio.gather(*set_tasks)
            await batcher.flush()
            set_time = time.perf_counter() - start

            print(f"\nðŸ“Š Redis Pipeline Batching:")
            print(f"   Set 100 keys in {set_time*1000:.2f}ms")
            print(f"   Avg: {set_time*10:.2f}Î¼s per operation")

            # Get operations (batched)
            start = time.perf_counter()
            get_tasks = [batcher.get(key) for key in keys]
            results = await asyncio.gather(*get_tasks)
            await batcher.flush()
            get_time = time.perf_counter() - start

            print(f"   Get 100 keys in {get_time*1000:.2f}ms")

            # Check results
            assert len(results) == 100
            assert all(r is not None for r in results)

            stats = batcher.get_stats()
            print(f"   Total operations: {stats['total_operations']}")
            print(f"   Flushes: {stats['flushes']}")
            print(f"   Avg batch size: {stats['avg_batch_size']:.1f}")
            print(f"   Latency saved: {stats['total_latency_saved_ms']:.0f}ms")

            # Cleanup
            await redis_client.delete(*keys)
            await redis_client.close()

        except Exception as e:
            pytest.skip(f"Redis not available: {e}")


class TestProcessPoolWarmup:
    """Test process pool warmup"""

    @pytest.mark.asyncio
    async def test_pool_warmup_basic(self):
        """
        Test process pool warmup
        Should eliminate 500ms cold start
        """
        from subzero.services.orchestrator.pool_warmup import ProcessPoolWarmer, _warmup_hash_operations

        warmer = ProcessPoolWarmer()

        # Add pool with warmup tasks
        warmer.add_pool(name="test_pool", max_workers=2, warmup_tasks=[_warmup_hash_operations], warmup_iterations=3)

        # Warmup
        start = time.perf_counter()
        await warmer.warmup_all()
        warmup_time = time.perf_counter() - start

        print(f"\nðŸ“Š Process Pool Warmup:")
        print(f"   Warmup time: {warmup_time*1000:.0f}ms")

        stats = warmer.get_stats()
        print(f"   Pools: {stats['total_pools']}")
        print(f"   Workers: {stats['total_workers']}")

        # Get executor and test it's ready
        executor = warmer.get_executor("test_pool")
        assert executor is not None

        # Execute task (should be instant, no cold start)
        loop = asyncio.get_running_loop()
        start = time.perf_counter()
        result = await loop.run_in_executor(executor, _warmup_hash_operations)
        exec_time = time.perf_counter() - start

        print(f"   First execution: {exec_time*1000:.0f}ms (no cold start!)")

        assert exec_time < 0.1, f"Execution too slow: {exec_time*1000:.0f}ms"

        await warmer.shutdown()


class TestIntegratedPerformance:
    """Test all optimizations together"""

    @pytest.mark.asyncio
    async def test_integrated_performance(self):
        """
        Test combined performance of all optimizations

        Expected: 40-60% overall improvement
        """
        from subzero.services.auth.shared_memory_cache import SharedMemoryCache
        from subzero.services.concurrency.backpressure import BackpressureManager, ServiceLimits

        # Setup shared memory cache
        cache = SharedMemoryCache(max_tokens=1000)

        # Setup backpressure
        manager = BackpressureManager()
        manager.register_service("cache", max_concurrent=50)

        # Write tokens
        for i in range(100):
            cache.write_token(user_id=i, token_hash=123456789 + i, expires_at=time.time() + 3600, scopes={0, 1})

        # Read with backpressure
        async def read_token(slot: int):
            async with manager.limit("cache"):
                return cache.read_token(slot)

        start = time.perf_counter()
        tasks = [read_token(i) for i in range(100)]
        results = await asyncio.gather(*tasks)
        total_time = time.perf_counter() - start

        print(f"\nðŸ“Š Integrated Performance:")
        print(f"   100 cache reads with backpressure in {total_time*1000:.2f}ms")
        print(f"   Throughput: {100/total_time:.0f} ops/sec")

        assert len(results) == 100
        assert sum(1 for r in results if r is not None) >= 95

        cache.close()
